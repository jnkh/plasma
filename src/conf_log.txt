BEST PERFORMANCE YET: ROC 94.6, crossing point 8%
target = t.HingeTarget

conf = {
   'data': {
        'use_shots' : 200000,
        'dt' : 0.001,
        'T_max' : 1000.0,
        'T_warning' : 1.0, #The shortest works best so far: less overfitting. log TTd prediction also works well. 0.5 better than 0.2
        'current_thresh' : 750000,
        'target' : target,
        'normalizer' : 'var',           #TODO optimize
   },
   'model': {
        'pred_length' : 200,
        'pred_batch_size' : 128,
        'length' : 128,                     #TODO optimize
        'skip' : 1,
        #hidden layer size
        'rnn_size' : 100,                   #TODO optimize
        #size 100 slight overfitting, size 20 no overfitting. 200 is not better than 100. Prediction much better with size 100, size 20 cannot capture the data.
        'rnn_type' : 'LSTM',
        'rnn_layers' : 3,                   #TODO optimize
        # 'output_activation' : target.activation,
        'optimizer' : 'adam', #have not found a difference yet
        'clipnorm' : 10.0,
        'regularization' : 0.0,#5e-6,#0.00001,
        # 'loss' : target.loss, #binary crossentropy performs slightly better?
        'lr' : 5e-5,#1e-4 is too high, 1e-7 is too low. 5e-5 seems best at 256 batch size, full dataset and ~10 epochs.
        'lr_decay' : 0.9,
        'stateful' : True,
        'return_sequences' : True,
        'dropout_prob' : 0.3,
    },

    'training': {
        'as_array_of_shots':True,
        'shuffle_training' : True,
        'train_frac' : 0.5,
        'validation_frac' : 0.05,
        'batch_size' : 256, #100
        'max_patch_length' : 100000, #THIS WAS THE CULPRIT FOR NO TRAINING! Lower than 1000 performs very poorly
        'num_shots_at_once' :  200, #How many shots are we loading at once?
        'num_epochs' : 10,
   },
}


Best Performance Yet: ROC 95.5, crossing point ~7%
Distributed training over 20 nodes.

{'data': {'T_max': 1000.0,
          'T_warning': 1.0,
          'current_index': 0,
          'current_thresh': 750000,
          'dt': 0.001,
          'normalizer': 'var',
          'num_signals': 8,
          'plotting': False,
          'recompute': False,
          'recompute_normalization': False,
          'target': <class 'targets.HingeTarget'>,
          'use_shots': 200000,
          'window_decay': 2,
          'window_size': 10},
 'model': {'clipnorm': 10.0,
           'dropout_prob': 0.3,
           'length': 128,
           'lr': 0.0005,
           'lr_decay': 0.5,
           'optimizer': 'adam',
           'pred_batch_size': 128,
           'pred_length': 200,
           'regularization': 0.0,
           'return_sequences': True,
           'rnn_layers': 3,
           'rnn_size': 300,
           'rnn_type': 'LSTM',
           'skip': 1,
           'stateful': True},
 'paths': {'base_path': '/tigress/jk7/',
           'model_save_path': './tmp/',
           'normalizer_path': '/tigress/jk7/data/normalization/normalization.npz',
           'processed_prepath': '/tigress/jk7/data/processed_shots/',
           'results_prepath': '/tigress/jk7/data/results/',
           'shot_files': ['CWall_clear.txt', 'CFC_unint.txt'],
           'shot_files_test': ['BeWall_clear.txt', 'ILW_unint.txt'],
           'shot_list_dir': '/tigress/jk7/data/shot_lists/',
           'signal_prepath': '/tigress/jk7/data/signal_data/jet/',
           'signals_dirs': ['jpf/da/c2-ipla',
                            'jpf/da/c2-loca',
                            'jpf/db/b5r-ptot>out',
                            'jpf/df/g1r-lid:003',
                            'jpf/gs/bl-li<s',
                            'jpf/gs/bl-fdwdt<s',
                            'jpf/gs/bl-ptot<s',
                            'jpf/gs/bl-wmhd<s']},
 'training': {'as_array_of_shots': True,
              'batch_size': 256,
              'data_parallel': False,
              'max_patch_length': 100000,
              'num_epochs': 13,
              'num_shots_at_once': 200,
              'shuffle_training': True,
              'train_frac': 0.5,
              'use_mock_data': False,
              'validation_frac': 0.05}}